![Tests](https://github.com/HectorMartinDama/spark-log-analytics/workflows/Tests%20and%20CI/badge.svg)
![Coverage](https://img.shields.io/badge/coverage-85%25-green)



# ğŸš€ Web Log Analytics Pipeline - PySpark

> **Pipeline ETL distribuido para anÃ¡lisis de logs web con Apache Spark**

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5-orange)](https://spark.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

## ğŸ“‹ DescripciÃ³n del Proyecto

Sistema de anÃ¡lisis de logs web implementado con **PySpark** que simula un caso real de **Data Engineering en producciÃ³n**. El proyecto implementa un pipeline ETL completo con:

- âœ… **Procesamiento distribuido** de grandes volÃºmenes de datos
- âœ… **Transformaciones complejas** con agregaciones y window functions
- âœ… **Data Quality checks** y validaciÃ³n de datos
- âœ… **DetecciÃ³n de anomalÃ­as** en tiempo de procesamiento
- âœ… **OptimizaciÃ³n de queries** con particionamiento inteligente
- âœ… **Formato Parquet** para almacenamiento en columna eficiente

### ğŸ¯ Casos de Uso

1. **AnÃ¡lisis de trÃ¡fico web**: Identificar patrones horarios y dÃ­as pico
2. **MonitorizaciÃ³n de rendimiento**: Detectar endpoints lentos o con errores
3. **Seguridad**: Identificar IPs sospechosas con alto volumen de requests
4. **Business Intelligence**: MÃ©tricas de negocio sobre uso de APIs vs Web

---

## ğŸ—ï¸ Arquitectura del Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXTRACT   â”‚ â”€â”€â”€> â”‚    CLEAN     â”‚ â”€â”€â”€> â”‚  TRANSFORM  â”‚ â”€â”€â”€> â”‚   LOAD   â”‚
â”‚             â”‚      â”‚              â”‚      â”‚             â”‚      â”‚          â”‚
â”‚ â€¢ Raw Logs  â”‚      â”‚ â€¢ Validation â”‚      â”‚ â€¢ Aggreg.   â”‚      â”‚ â€¢ Parquetâ”‚
â”‚ â€¢ S3/HDFS   â”‚      â”‚ â€¢ Parsing    â”‚      â”‚ â€¢ Analytics â”‚      â”‚ â€¢ Delta  â”‚
â”‚ â€¢ Streaming â”‚      â”‚ â€¢ Quality    â”‚      â”‚ â€¢ Anomalies â”‚      â”‚ â€¢ S3     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Clave

1. **Extract**: GeneraciÃ³n/Lectura de logs (simulaciÃ³n de S3/Kafka)
2. **Clean**:
   - Parseo de timestamps
   - DetecciÃ³n de bots
   - ClasificaciÃ³n de endpoints (API/WEB)
   - Filtrado de registros invÃ¡lidos
3. **Transform**:
   - Agregaciones temporales (hora, dÃ­a)
   - MÃ©tricas por endpoint
   - AnÃ¡lisis de rendimiento
   - Top IPs sospechosas
4. **Anomalies Detection**:
   - Endpoints con alto error rate
   - Response times anÃ³malos (>3Ïƒ)
5. **Load**: Persistencia en formato Parquet optimizado

---

## ğŸš€ Quick Start

### Prerequisitos

```bash
# Python 3.8+
python --version

# Java 8 o 11 (requerido por Spark)
java -version
```

### InstalaciÃ³n

```bash
# 1. Clonar el repositorio
git clone https://github.com/HectorMartinDama/spark-log-analytics.git
cd spark-log-analytics

# 2. Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# 3. Instalar dependencias
pip install pyspark==4.0.1
pip install pandas  # opcional, para anÃ¡lisis adicional

# 4. Ejecutar el pipeline
python web_log_pipeline.py
```
---

## ğŸ“Š Resultados del AnÃ¡lisis

### 1. TrÃ¡fico por Hora del DÃ­a

```
+------------+--------------+------------------+-----------+
|hour_of_day |total_requests|avg_response_time |error_count|
+------------+--------------+------------------+-----------+
|0           |342           |1523.4            |12         |
|1           |298           |1487.2            |8          |
...
```

### 2. Top Endpoints

```
+------------------+-------------+--------------+------------------+------------+
|endpoint          |endpoint_type|total_requests|avg_response_time |success_rate|
+------------------+-------------+--------------+------------------+------------+
|/api/products     |API          |1523          |1234.5            |98.2        |
|/home             |WEB          |1456          |876.3             |99.1        |
...
```

### 3. AnomalÃ­as Detectadas

```
âš ï¸  Se detectaron 3 endpoints con comportamiento anÃ³malo
+------------------+--------------+------------------+----------+
|endpoint          |request_count |avg_response_time |error_rate|
+------------------+--------------+------------------+----------+
|/admin            |234           |4532.1            |23.5      |
```

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Lectura desde S3 (ProducciÃ³n)

```python
# Configurar credenciales AWS
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "YOUR_KEY")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "YOUR_SECRET")

# Leer logs desde S3
df = spark.read.json("s3a://your-bucket/logs/year=2024/month=12/*")
```

### IntegraciÃ³n con Databricks

```python
# En Databricks Notebook
from web_log_pipeline import WebLogAnalyticsPipeline

# Leer desde Delta Lake
df = spark.read.format("delta").load("/mnt/logs/web_logs")

# Ejecutar pipeline
pipeline = WebLogAnalyticsPipeline()
results = pipeline.transform_and_aggregate(df)

# Guardar en Delta
results["endpoint_stats"] \
    .write.format("delta") \
    .mode("overwrite") \
    .save("/mnt/analytics/endpoint_stats")
```

---

## ğŸ“ˆ MÃ©tricas de Rendimiento

| MÃ©trica              | Valor                       |
| -------------------- | --------------------------- |
| Registros procesados | 10,000                      |
| Tiempo de ejecuciÃ³n  | ~15 segundos                |
| Particiones Spark    | 4 (optimizado)              |
| Formato salida       | Parquet (compresiÃ³n snappy) |
| ReducciÃ³n de tamaÃ±o  | ~70% vs CSV                 |

---

## ğŸ§ª Testing

```bash
# Ejecutar con dataset de prueba
python web_log_pipeline.py --num-records 1000

# Modo debug
python web_log_pipeline.py --log-level DEBUG
```

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Apache Spark**: Motor de procesamiento distribuido
- **PySpark**: API de Python para Spark
- **Parquet**: Formato columnar de almacenamiento
- **Delta Lake** (opcional): GestiÃ³n de data lakes con ACID
- **Pandas** (opcional): AnÃ¡lisis complementario

---

---

## ğŸ“ Conceptos de Data Engineering Demostrados

âœ… **ETL Pipelines**: ExtracciÃ³n, TransformaciÃ³n y Carga de datos  
âœ… **Distributed Computing**: Procesamiento paralelo con Spark  
âœ… **Data Quality**: ValidaciÃ³n y limpieza de datos  
âœ… **Aggregations**: Group by, window functions, joins  
âœ… **Optimization**: Partitioning, caching, broadcast joins  
âœ… **Anomaly Detection**: EstadÃ­stica descriptiva y umbrales

## ğŸ“„ Licencia

MIT License - Ver [LICENSE](LICENSE) para mÃ¡s detalles

---

## ğŸ‘¤ Autor

**HÃ©ctor MartÃ­n**

- ğŸ’¼ LinkedIn: [HectorMartinDama]
- ğŸ™ GitHub: [@HectorMartinDama]
- ğŸ“§ Email: hectormartindama@gmail.com

---

## ğŸ™ Agradecimientos

Proyecto creado como demostraciÃ³n de habilidades en **Data Engineering con PySpark** para procesos de selecciÃ³n en el sector tech.

---

**â­ Si te resultÃ³ Ãºtil este proyecto, considera darle una estrella en GitHub**
