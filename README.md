![Tests](https://github.com/HectorMartinDama/spark-log-analytics/workflows/Tests%20and%20CI/badge.svg)
![Coverage](https://img.shields.io/badge/coverage-85%25-green)



# üöÄ Web Log Analytics Pipeline - PySpark

> **Pipeline ETL distribuido para an√°lisis de logs web con Apache Spark**

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-4.0.1-orange)](https://spark.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

## üìã Descripci√≥n del Proyecto

Sistema de an√°lisis de logs web implementado con **PySpark** que simula un caso real de **Data Engineering en producci√≥n**. El proyecto implementa un pipeline ETL completo con:

- ‚úÖ **Procesamiento distribuido** de grandes vol√∫menes de datos
- ‚úÖ **Transformaciones complejas** con agregaciones y window functions
- ‚úÖ **Data Quality checks** y validaci√≥n de datos
- ‚úÖ **Detecci√≥n de anomal√≠as** en tiempo de procesamiento
- ‚úÖ **Optimizaci√≥n de queries** con particionamiento inteligente
- ‚úÖ **Formato Parquet** para almacenamiento en columna eficiente

### üéØ Casos de Uso

1. **An√°lisis de tr√°fico web**: Identificar patrones horarios y d√≠as pico
2. **Monitorizaci√≥n de rendimiento**: Detectar endpoints lentos o con errores
3. **Seguridad**: Identificar IPs sospechosas con alto volumen de requests
4. **Business Intelligence**: M√©tricas de negocio sobre uso de APIs vs Web

---

## üèóÔ∏è Arquitectura del Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   EXTRACT   ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ    CLEAN     ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ  TRANSFORM  ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ   LOAD   ‚îÇ
‚îÇ             ‚îÇ      ‚îÇ              ‚îÇ      ‚îÇ             ‚îÇ      ‚îÇ          ‚îÇ
‚îÇ ‚Ä¢ Raw Logs  ‚îÇ      ‚îÇ ‚Ä¢ Validation ‚îÇ      ‚îÇ ‚Ä¢ Aggreg.   ‚îÇ      ‚îÇ ‚Ä¢ Parquet‚îÇ
‚îÇ ‚Ä¢ S3/HDFS   ‚îÇ      ‚îÇ ‚Ä¢ Parsing    ‚îÇ      ‚îÇ ‚Ä¢ Analytics ‚îÇ      ‚îÇ ‚Ä¢ Delta  ‚îÇ
‚îÇ ‚Ä¢ Streaming ‚îÇ      ‚îÇ ‚Ä¢ Quality    ‚îÇ      ‚îÇ ‚Ä¢ Anomalies ‚îÇ      ‚îÇ ‚Ä¢ S3     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes Clave

1. **Extract**: Generaci√≥n/Lectura de logs (simulaci√≥n de S3/Kafka)
2. **Clean**:
   - Parseo de timestamps
   - Detecci√≥n de bots
   - Clasificaci√≥n de endpoints (API/WEB)
   - Filtrado de registros inv√°lidos
3. **Transform**:
   - Agregaciones temporales (hora, d√≠a)
   - M√©tricas por endpoint
   - An√°lisis de rendimiento
   - Top IPs sospechosas
4. **Anomalies Detection**:
   - Endpoints con alto error rate
   - Response times an√≥malos (>3œÉ)
5. **Load**: Persistencia en formato Parquet optimizado

---

## üöÄ Quick Start

### Prerequisitos

```bash
# Python 3.8+
python --version

# Java 8 o 11 (requerido por Spark)
java -version
```

### Instalaci√≥n

```bash
# 1. Clonar el repositorio
git clone https://github.com/HectorMartinDama/spark-log-analytics.git
cd spark-log-analytics

# 2. Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# 3. Instalar dependencias
pip install pyspark==4.0.1
pip install pandas  # opcional, para an√°lisis adicional

# 4. Ejecutar el pipeline
python web_log_pipeline.py
```
---

## üìä Resultados del An√°lisis

### 1. Tr√°fico por Hora del D√≠a

```
+------------+--------------+------------------+-----------+
|hour_of_day |total_requests|avg_response_time |error_count|
+------------+--------------+------------------+-----------+
|0           |342           |1523.4            |12         |
|1           |298           |1487.2            |8          |
...
```

### 2. Top Endpoints

```
+------------------+-------------+--------------+------------------+------------+
|endpoint          |endpoint_type|total_requests|avg_response_time |success_rate|
+------------------+-------------+--------------+------------------+------------+
|/api/products     |API          |1523          |1234.5            |98.2        |
|/home             |WEB          |1456          |876.3             |99.1        |
...
```

### 3. Anomal√≠as Detectadas

```
‚ö†Ô∏è  Se detectaron 3 endpoints con comportamiento an√≥malo
+------------------+--------------+------------------+----------+
|endpoint          |request_count |avg_response_time |error_rate|
+------------------+--------------+------------------+----------+
|/admin            |234           |4532.1            |23.5      |
```

---

## üîß Configuraci√≥n Avanzada

### Lectura desde S3 (Producci√≥n)

```python
# Configurar credenciales AWS
spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", "YOUR_KEY")
spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "YOUR_SECRET")

# Leer logs desde S3
df = spark.read.json("s3a://your-bucket/logs/year=2024/month=12/*")
```

### Integraci√≥n con Databricks

```python
# En Databricks Notebook
from web_log_pipeline import WebLogAnalyticsPipeline

# Leer desde Delta Lake
df = spark.read.format("delta").load("/mnt/logs/web_logs")

# Ejecutar pipeline
pipeline = WebLogAnalyticsPipeline()
results = pipeline.transform_and_aggregate(df)

# Guardar en Delta
results["endpoint_stats"] \
    .write.format("delta") \
    .mode("overwrite") \
    .save("/mnt/analytics/endpoint_stats")
```

---

## üìà M√©tricas de Rendimiento

| M√©trica              | Valor                       |
| -------------------- | --------------------------- |
| Registros procesados | 10,000                      |
| Tiempo de ejecuci√≥n  | ~15 segundos                |
| Particiones Spark    | 4 (optimizado)              |
| Formato salida       | Parquet (compresi√≥n snappy) |
| Reducci√≥n de tama√±o  | ~70% vs CSV                 |

---

## üß™ Testing

```bash
# Ejecutar con dataset de prueba
python web_log_pipeline.py --num-records 1000

# Modo debug
python web_log_pipeline.py --log-level DEBUG
```

---

## üõ†Ô∏è Tecnolog√≠as Utilizadas

- **Apache Spark**: Motor de procesamiento distribuido
- **PySpark**: API de Python para Spark
- **Parquet**: Formato columnar de almacenamiento
- **Delta Lake** (opcional): Gesti√≥n de data lakes con ACID
- **Pandas** (opcional): An√°lisis complementario

---

---

## üéì Conceptos de Data Engineering Demostrados

‚úÖ **ETL Pipelines**: Extracci√≥n, Transformaci√≥n y Carga de datos  
‚úÖ **Distributed Computing**: Procesamiento paralelo con Spark  
‚úÖ **Data Quality**: Validaci√≥n y limpieza de datos  
‚úÖ **Aggregations**: Group by, window functions, joins  
‚úÖ **Optimization**: Partitioning, caching, broadcast joins  
‚úÖ **Anomaly Detection**: Estad√≠stica descriptiva y umbrales

---

## üë§ Autor

**H√©ctor Mart√≠n**

- üíº LinkedIn: [HectorMartinDama]
- üêô GitHub: [@HectorMartinDama]
- üìß Email: hectormartindama@gmail.com

---

## üôè Agradecimientos

Proyecto creado como demostraci√≥n de habilidades en **Data Engineering con PySpark** para procesos de selecci√≥n en el sector tech.

---

**‚≠ê Si te result√≥ √∫til este proyecto, considera darle una estrella en GitHub**
