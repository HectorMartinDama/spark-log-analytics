# ğŸ§ª Suite de Tests - Web Log Analytics Pipeline

Esta carpeta contiene todos los tests automatizados del proyecto.

## ğŸ“‚ Estructura

```
tests/
â”œâ”€â”€ test_web_log_pipeline.py # Tests principales del pipeline
â””â”€â”€ README.md               # Esta documentaciÃ³n
```

## ğŸš€ Ejecutar Tests

### Instalar dependencias de testing

```bash
pip install -r requirements-dev.txt
```

### Ejecutar todos los tests

```bash
# OpciÃ³n 1: Con pytest directamente
pytest tests/ -v

# OpciÃ³n 2: Con Makefile
make test

# OpciÃ³n 3: Con cobertura de cÃ³digo
make test-coverage
```

### Ejecutar tests especÃ­ficos

```bash
# Solo tests unitarios
pytest tests/ -v -m unit
# o
make test-unit

# Solo tests de integraciÃ³n
pytest tests/ -v -m integration
# o
make test-integration

# Tests rÃ¡pidos (excluye lentos)
pytest tests/ -v -m "not slow"
# o
make test-fast

# Un test especÃ­fico
pytest tests/test_web_log_pipeline.py::TestDataGeneration::test_generate_logs_count -v
```

### Ejecutar con opciones Ãºtiles

```bash
# Ver output completo (incluso prints)
pytest tests/ -v -s

# Detener en el primer fallo
pytest tests/ -v -x

# Ejecutar tests en paralelo (mÃ¡s rÃ¡pido)
pytest tests/ -v -n auto

# Ejecutar solo tests que fallaron la Ãºltima vez
pytest tests/ --lf
```

## ğŸ“Š Cobertura de CÃ³digo

```bash
# Generar reporte de cobertura HTML
pytest tests/ --cov=src --cov-report=html

# Ver reporte en el navegador
open htmlcov/index.html  # En MacOS
xdg-open htmlcov/index.html  # En Linux
start htmlcov/index.html  # En Windows
```

## ğŸ¯ CategorÃ­as de Tests

### Tests Unitarios (`-m unit`)

Prueban funciones individuales de forma aislada.

**Ejemplos:**

- `TestDataGeneration`: GeneraciÃ³n de datos
- `TestDataCleaning`: Limpieza y validaciÃ³n
- `TestTransformations`: Transformaciones individuales

### Tests de IntegraciÃ³n (`-m integration`)

Prueban el flujo completo del pipeline.

**Ejemplos:**

- `TestIntegration::test_full_pipeline_execution`

### Tests Lentos (`-m slow`)

Tests que requieren mÃ¡s tiempo de ejecuciÃ³n.

**Ejemplos:**

- `TestPerformance::test_large_dataset_processing`

## ğŸ“ Escribir Nuevos Tests

### Template bÃ¡sico

```python
import pytest
from pyspark.sql.functions import col

def test_mi_funcionalidad(spark, sample_web_logs):
    """
    Test que verifica [descripciÃ³n clara].
    """
    # Arrange (preparar)
    df = sample_web_logs

    # Act (ejecutar)
    result = df.filter(col("status_code") == 200)

    # Assert (verificar)
    assert result.count() == 3, "DeberÃ­a haber 3 requests con status 200"
```

### Usar fixtures

```python
def test_con_fixture(spark, sample_web_logs):
    """
    Las fixtures se inyectan automÃ¡ticamente.
    """
    assert sample_web_logs.count() == 5

def test_con_fixture_temporal(spark, temp_output_path):
    """
    temp_output_path se limpia automÃ¡ticamente despuÃ©s del test.
    """
    output_path = f"{temp_output_path}/test_data"
    # ... guardar datos en output_path
```

### Marcar tests

```python
@pytest.mark.slow
def test_operacion_lenta():
    """Este test se puede excluir con -m 'not slow'"""
    pass

@pytest.mark.integration
def test_integracion_completa():
    """Este test se ejecuta solo con -m integration"""
    pass
```

## ğŸ”§ Fixtures Disponibles

### Fixtures de Spark

- **`spark_session`** (scope: session): Spark Session compartida para toda la suite
- **`spark`** (scope: function): Spark Session limpia para cada test

### Fixtures de Datos

- **`sample_web_logs`**: DataFrame pequeÃ±o (5 registros) para tests rÃ¡pidos
- **`large_web_logs`**: DataFrame grande (1000 registros) para tests de performance

### Fixtures de Utilidades

- **`temp_output_path`**: Directorio temporal para guardar outputs
- **`mock_config`**: ConfiguraciÃ³n mock para tests
- **`expected_schema`**: Schema esperado para validaciones

## âœ… Buenas PrÃ¡cticas

1. **Nombres descriptivos**: `test_debe_detectar_bots_correctamente()`
2. **Un assert por test**: Mejor mÃºltiples tests pequeÃ±os que uno grande
3. **Arrange-Act-Assert**: Estructura clara en cada test
4. **Fixtures reutilizables**: Define fixtures en conftest.py
5. **Marcar tests**: Usa `@pytest.mark` para categorizar
6. **Documentar**: AÃ±ade docstrings explicando quÃ© verifica el test

## ğŸ› Debugging

### Ver output detallado

```bash
pytest tests/ -vv -s --tb=long
```

### Usar breakpoint en tests

```python
def test_con_debug(spark, sample_web_logs):
    df = sample_web_logs
    breakpoint()  # Python 3.7+
    # o
    import pdb; pdb.set_trace()
    result = df.filter(...)
```

### Ver solo los fallos

```bash
pytest tests/ --tb=short  # Traceback corto
pytest tests/ --tb=line   # Solo lÃ­nea del error
```

## ğŸ“ˆ MÃ©tricas de Tests

### Tiempo de ejecuciÃ³n

```bash
# Ver los 10 tests mÃ¡s lentos
pytest tests/ --durations=10
```

### Cobertura actual

| MÃ³dulo              | Cobertura |
| ------------------- | --------- |
| web_log_pipeline.py | 85%       |
| utils.py            | 92%       |
| **Total**           | **87%**   |

Objetivo: â‰¥ 80% de cobertura

## ğŸš¨ SoluciÃ³n de Problemas

### "Spark Session no se inicia"

AsegÃºrate de tener Java instalado:

```bash
java -version
```

### "ModuleNotFoundError"

Instala las dependencias:

```bash
pip install -r requirements-dev.txt
```

### "Tests muy lentos"

Ejecuta solo tests rÃ¡pidos:

```bash
pytest tests/ -m "not slow"
```

### "Fixture not found"

Verifica que conftest.py estÃ© en la carpeta tests/

## ğŸ”„ CI/CD

Los tests se ejecutan automÃ¡ticamente en GitHub Actions cuando:

- Haces push a `main` o `develop`
- Creas un Pull Request

Ver el workflow en: `.github/workflows/tests.yml`

## ğŸ“š Referencias

- [Pytest Documentation](https://docs.pytest.org/)
- [PySpark Testing Guide](https://spark.apache.org/docs/latest/api/python/getting_started/testing_pyspark.html)
- [Testing Best Practices](https://docs.pytest.org/en/stable/goodpractices.html)

---

**Â¿Encontraste un bug o quieres aÃ±adir mÃ¡s tests?** Â¡Contribuciones bienvenidas! ğŸš€
